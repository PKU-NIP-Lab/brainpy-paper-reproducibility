{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install jax[tpu]==0.4.11 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import jax\n",
    "print(jax.__version__)\n",
    "print(jax.devices())"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install brainpy"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "import numpy as np\n",
    "\n",
    "bm.set_dt(0.1)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "taum = 20\n",
    "taue = 5\n",
    "taui = 10\n",
    "Vt = -50\n",
    "Vr = -60\n",
    "El = -60\n",
    "Erev_exc = 0.\n",
    "Erev_inh = -80.\n",
    "Ib = 20.\n",
    "ref = 5.0\n",
    "we = 0.6\n",
    "wi = 6.7"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LIF(bp.dyn.NeuDyn):\n",
    "    def __init__(self, size, V_init: callable, sharding=None):\n",
    "        super(LIF, self).__init__(size=size, sharding=sharding)\n",
    "\n",
    "        # parameters\n",
    "        self.V_rest = Vr\n",
    "        self.V_reset = El\n",
    "        self.V_th = Vt\n",
    "        self.tau = taum\n",
    "        self.tau_ref = ref\n",
    "\n",
    "        # variables\n",
    "        self.V = self.init_variable(V_init, self.mode)\n",
    "        self.spike = self.init_variable(lambda s: bm.zeros(s, dtype=bool), self.mode)\n",
    "        self.t_last_spike = self.init_variable(lambda s: bm.ones(s) * -1e7, self.mode)\n",
    "\n",
    "    def update(self, inp):\n",
    "        inp = self.sum_inputs(self.V.value, init=inp)  # sum all projection inputs\n",
    "        refractory = (bp.share['t'] - self.t_last_spike) <= self.tau_ref\n",
    "        V = self.V + (-self.V + self.V_rest + inp) / self.tau * bp.share['dt']\n",
    "        V = bm.where(refractory, self.V, V)\n",
    "        spike = self.V_th <= V\n",
    "        self.t_last_spike.value = bm.where(spike, bp.share['t'], self.t_last_spike)\n",
    "        self.V.value = bm.where(spike, self.V_reset, V)\n",
    "        self.spike.value = spike\n",
    "        return spike"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MaskedLinear(bp.dnn.Layer):\n",
    "  def __init__(self, num_pre, num_post, prob, weight, sharding=None):\n",
    "    super().__init__()\n",
    "    print('Using masked linear')\n",
    "    self.weight = weight\n",
    "    f = bm.jit(\n",
    "        lambda key: jax.random.bernoulli(key, prob, (num_pre, num_post)),\n",
    "        out_shardings=bm.sharding.get_sharding(sharding),\n",
    "    )\n",
    "    self.mask = f(bm.random.split_key())\n",
    "\n",
    "  def update(self, x):\n",
    "    return (x @ self.mask) * self.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Exponential(bp.Projection):\n",
    "  def __init__(self, num_pre, post, prob, g_max, tau, E):\n",
    "    super().__init__()\n",
    "    self.proj = bp.dyn.ProjAlignPostMg1(\n",
    "      comm=MaskedLinear(num_pre, post.num, prob, g_max, sharding=[None, bm.sharding.NEU_AXIS]),\n",
    "      syn=bp.dyn.Expon.desc(post.num, tau=tau, sharding=[bm.sharding.NEU_AXIS]),\n",
    "      out=bp.dyn.COBA.desc(E=E),\n",
    "      post=post\n",
    "    )\n",
    "\n",
    "  def update(self, spk):\n",
    "    spk = bm.asarray(spk, dtype=float)\n",
    "    self.proj.update(spk)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class COBA(bp.DynSysGroup):\n",
    "    def __init__(self, scale, monitor=False):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.num_exc = int(3200 * scale)\n",
    "        self.num_inh = int(800 * scale)\n",
    "        self.N = LIF(self.num_exc + self.num_inh, V_init=bp.init.Normal(-55., 5.))\n",
    "        self.E = Exponential(self.num_exc, self.N, prob=80. / self.N.num, E=Erev_exc, g_max=we, tau=taue)\n",
    "        self.I = Exponential(self.num_inh, self.N, prob=80. / self.N.num, E=Erev_inh, g_max=wi, tau=taui)\n",
    "\n",
    "    def update(self, inp=Ib):\n",
    "        self.E(self.N.spike[:self.num_exc])\n",
    "        self.I(self.N.spike[self.num_exc:])\n",
    "        self.N(inp)\n",
    "        if self.monitor:\n",
    "            return self.N.spike.value"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def run_a_simulation2(scale=10, duration=1e3, platform='cpu', x64=True, monitor=False):\n",
    "  bm.set_platform(platform)\n",
    "  bm.random.seed()\n",
    "  if x64:\n",
    "    bm.enable_x64()\n",
    "\n",
    "  net = COBA(scale=scale, monitor=monitor)\n",
    "\n",
    "  @bm.jit\n",
    "  def run(indices):\n",
    "    return bm.for_loop(net.step_run, indices, progress_bar=False)\n",
    "\n",
    "  indices = np.arange(int(duration / bm.get_dt()))\n",
    "  t0 = time.time()\n",
    "  r = jax.block_until_ready(run(indices))\n",
    "  t1 = time.time()\n",
    "  print(f'first run time = {t1 - t0} s')\n",
    "\n",
    "  indices = np.arange(int(duration / bm.get_dt()), int(duration / bm.get_dt()) * 2)\n",
    "  t2 = time.time()\n",
    "  r = jax.block_until_ready(run(indices))\n",
    "  t3 = time.time()\n",
    "  jax.debug.visualize_array_sharding(r)\n",
    "  print(f'second run time = {t3 - t2} s')\n",
    "\n",
    "  # running\n",
    "  if monitor:\n",
    "    r = jax.device_put(r, jax.devices('cpu')[0])\n",
    "    r = bm.as_numpy(r)\n",
    "    print(f'scale={scale}, size={net.num}, first run time = {t1 - t0} s, second run time = {t3 - t2} s, '\n",
    "          f'firing rate = {r.sum() / net.num / duration * 1e3} Hz')\n",
    "  else:\n",
    "    print(f'scale={scale}, size={net.num}, first run time = {t1 - t0} s, second run time = {t3 - t2} s')\n",
    "  bm.disable_x64()\n",
    "  bm.clear_buffer_memory(platform)\n",
    "  return net.N.num, t1 - t0, t3 - t2"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with bm.sharding.device_mesh(jax.devices(), [bm.sharding.NEU_AXIS]):\n",
    "    for s in [1, 2, 4, 6, 8, 10, 20, 40, 60, 80, 100]:\n",
    "      run_a_simulation2(scale=s, duration=5e3, platform='tpu', x64=False, monitor=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
